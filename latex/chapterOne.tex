%
% $Id: chapterOne.tex
%

% A first, optional argument in [ ] is the title as displayed in the table of contents
% The second argument is the title as displayed here.  Use \\ as appropriate in
%   this title to get desired line breaks

%----------------------------------------------------------------------------------------
% Introduction
%----------------------------------------------------------------------------------------
\chapter[Introduction]{Introduction}
\label{chp:intro}

The formation of compound waves in numerical solutions of the ideal \gls{mhd} equations is studied using dissipative finite volume schemes.  Numerical solutions containing compound waves was encountered during the development of a \gls{fv} \gls{fct} solver for ideal MHD capable of running on a \gls{gpu}.  The FV-FCT code was used to determine the benefits and limitations of GPU parallelism.  Chapter~\ref{chp:gpu} contains original results on GPU efficiency.  The first four chapters of this dissertation address compound wave solutions to Riemann problems of ideal MHD. 

A Riemann problem is a one-dimensional initial value problem for a conservative system in which a single discontinuity separates two constant states.  Riemann problems play an important role in fluid simulations; numerical algorithms in both computational fluid dynamics (CFD) and computational MHD use linear approximations of local Riemann problems for the computation of numerical fluxes \citep{Brio:1988,Ryu:1995a}.  A Riemann problem is properly defined, and solutions for different conservative systems are discussed in Chapter~\ref{chp:riemann}.  Solutions of Riemann problems are composed of multiple waves that emanate away from a discontinuity.  Exact and approximate solutions for the Euler equations of \gls{hd} are presented.   The exact solutions were obtained with a nonlinear solver for HD Euler equations, that was developed for this thesis.

The equations of ideal MHD is the last conservative system discussed in Chapter~\ref{chp:riemann}.  The ideal MHD equations are more complex than the Euler equations of hydrodynamics. As a result, the number of possible structures is greater for ideal MHD.  In addition, the system of equations is non-strictly hyperbolic, which makes non-regular structures such as intermediate shocks and compound waves possible.  

A solution is only considered physical if it satisfies entropy and evolutionary conditions \citep{Jeffrey:1964}.  The entropy is $p_g/\rho^{\gamma}$, where $p_g$ is the gas pressure, $\rho$ is the density, and $\gamma$ is the ratio of specific heats.  The entropy condition states that the change in entropy across a shock is zero or larger.  The evolutionary condition requires a shock to be structurally stable under small perturbations \citep{Jeffrey:1964}.  In hydrodynamics, the entropy and evolutionary conditions are equivalent.

In the past, intermediate shocks in ideal MHD have been considered unphysical because they are structurally unstable under small perturbations \citep{Landau:1984}.  In recent years, their physicality has been reconsidered. Observations of heliospheric plasma and numerical simulations of bow shocks have provided evidence for their existence. \citet{Feng:2008} reported that a discontinuity observed by \emph{Voyager} 2 in January 1979 was an intermediate shock. \citet{Chao:1993} identified an intermediate shock in \emph{Voyager} 1 measurements in 1980. Intermediate shocks have been observed in numerical simulations of bow shocks in both two- and three-dimensions \citep{DeSterck:1998,DeSterck:2000}.  They were first observed in numerical simulations by \citet{Brio:1988} whose results have been used extensively as a reference for numerical solutions of the ideal MHD equations.  The classification systems for intermediate, and normal Lax shocks is given in Chapter~\ref{chp:riemann}.

Chapter~\ref{chp:num_mhd} presents the exact and approximate solution to Riemann problems of ideal MHD.  An nonlinear solver for ideal MHD non-linear Riemann solver, that is based on the method described by \citet{Dai:1994a} with the rarefaction wave extension by \citet{Ryu:1995a}, was developed to provide the exact solution.  The nonlinear solver is also used to create original benchmarks for ideal MHD model development.  

Chapter~\ref{chp:num_mhd} also discusses non-uniform convergence exhibited by dissipative FV schemes.  \citet{Torrilhon:2003b} computed the convergence rates for various implementations of the finite volume method on one-dimensional Riemann problems with non-unique solutions. All implementations exhibited non-uniform convergence with respect to grid resolution.  The schemes produced solutions that converged toward the non-regular solution until a certain level of grid refinement, at which point convergence was to the regular solution.  This behavior was referred to as pseudo-convergence, and numerical diffusion was identified as the cause.  For the coplanar case, in which the rotation angle is 180$\dsym$, we argue that convergence to the non-regular solution is expected to always occur, independent of grid resolution, because the transverse velocity and magnetic field are restricted to a single plane.

Because grids with more than $10^4$ points are needed to obtain $L^1$ errors on the order of $10^{-2}$, Torrhilon \citep{Torrilhon:2003b} suggested using adaptive mesh refinement (AMR) to reduce the computational costs.  AMR can be a powerful computational tool but is complex to implement, and for structured grids, it introduces non-conformity.  

In Chapter~\ref{chp:cwm}, we present an alternative method for error reduction, Compound Wave Modification (CWM), that requires modifying the flux from the finite volume approximation.  The modification is done to the Harden-Lax-van Leer-Discontinuities (HLLD) \citep{Miyoshi:2005} approximate Riemann solver implemented in the \emph{Athena} MHD code \citep{Stone:2008,AthenaSite}.  The CWM solutions are compared with one-dimensional exact solutions for a near-coplanar case and the coplanar case.  Chapter~\ref{chp:cwm} is almost entirely original work.  After Chapter~\ref{chp:cwm}, the focus of the dissertation shifts from solutions of Riemann problems to high performance computing with graphics processing units (GPUs).

Numerical solutions for problems in ideal MHD can be computationally expensive to obtain.  Most large-scale simulations require some sort of parallelism, where data is processed simultaneously.  Parallelism occurs in two ways: distributed-, and shared-memory.  When parallelism is done by way of distributed-memory, the domain is partitioned geometrically and each subset is send to a separate processor where the solution is updated.  The process is known as domain decomposition.   After each update, the processors must communicate with one another to update the solution at the boundaries between sub-domains.  This is done by assigning one processor as the \emph{master} that handles communication.  After each update of the solution, the processors exchange information with the master to ensure the boundaries of each sub-domain are updated appropriately.

Parallelism for shared memory refers to simultaneous computation on a single processing unit, either CPU of GPU.  The standard approach is to invoke multiple threads on the CPU, each thread performing the same computations and updating the solution independently.  Shared memory parallelism does not suffer the overhead costs of communication between processor, but great must be taken to ensure that two or more threads do not attempt to access the same portion of memory simultaneously.  This issue is known as memory contention and it introduces errors into the computation.  The GPU offers an alternative option to the CPU for a shared memory device.  The GPU is capable of exceeding the computational power of a CPU by an order of magnitude.  The theoretical peak performance in \gls{flops} of the NVIDIA GeForce GTX Titan is 4500 GFLOPS for single percision, for the Xeon E5645 @ 2.4 GHz is 460 GFLOPS per sec for single precision.  The computational potential of the GPU is superior to the CPU, however, in order to take advantage of the increased computational power of the GPU, the data must be transferred from the host (CPU) to the device (GPU) and back to the host where it can be written to the drive.  We expect to see the increased performance gains from running on the GPU as the number of operations, e.g., addition, subtraction, etc., increase.  In chapter 5, we see the GPU performance increase from two to two and one half times as fast as the CPU using twelve core (twenty-four threads) when the HLLD approximate Riemann solver is used to compute the flux in place of the less computationally demanding Rusanov flux.

Chapter~\ref{chp:gpu} is a study of the two approaches to shared memory parallelism mentioned above, namely the CPU and GPU.  A multidimensional fluid solver capable of solving the Euler equations of hydrodynamics and the ideal MHD equations has been written for this dissertation and is available for download at: \protect\gitrepo.  The Trust \citep{Thrust} library has been utilized to implement shared memory parallelism.  Thrust is a \cpp parallel template library.  It supports four device backends: \gls{cuda}, \gls{omp}, \gls{tbb}, and the standard \cpp device for serial runs.  The CUDA backend utilizes the GPU, while the OMP and TBB backends utilize multi-core processing on the CPU.  The GPU is found to outperform the CPU running one core by one to two orders of magnitude and by two to three times when OMP is enabled.  Chapter~\ref{chp:gpu} deminstrates efficient memory access for the GPU by considering two approaches to data storage, \gls{aos} and \gls{soa}.  The SoA approach is shown to improve performance by one and a half to two times depending on the algorithm used.  
              




